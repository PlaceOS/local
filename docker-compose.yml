version: "3.7"

networks:
  placeos:
    name: placeos
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.31.231.0/24

volumes:
  elastic-data:
  influx-data:
  nginx-data:
  postgres-data:
  redis-data:
  core-repositories:
  core-drivers:
  www:

# YAML Anchors

x-cap-drop: &cap-drop
  cap_drop:
    # - AUDIT_WRITE      # Write records to kernel auditing log.
    - CHOWN            # Make arbitrary changes to file UIDs and GIDs (see chown(2)).
    - DAC_OVERRIDE     # Bypass file read, write, and execute permission checks.
    # - FOWNER           # Bypass permission checks on operations that normally require the file system UID of the process to match the UID of the file.
    # - FSETID           # Donâ€™t clear set-user-ID and set-group-ID permission bits when a file is modified.
    # - KILL             # Bypass permission checks for sending signals.
    - MKNOD            # Create special files using mknod(2).
    # - NET_BIND_SERVICE # Bind a socket to internet domain privileged ports (port numbers less than 1024).
    # - NET_RAW          # Use RAW and PACKET sockets.
    - SETFCAP          # Set file capabilities.
    - SETGID           # Make arbitrary manipulations of process GIDs and supplementary GID list.
    - SETPCAP          # Modify process capabilities.
    - SETUID           # Make arbitrary manipulations of process UIDs.
    - SYS_CHROOT       # Use chroot(2), change root directory.

x-deployment-env: &deployment-env
  LOG_LEVEL: ${LOG_LEVEL:-warn}
  ENV: ${ENV:-development}
  SG_ENV: ${SG_ENV:-development}
  TZ: $TZ

x-edge-env: &edge-env
  PLACE_EDGE_KEY: ${PLACE_EDGE_KEY:-}
  PLACE_URI: ${PLACE_URI:-http://api:3000}

x-jwt-public-key-env: &jwt-public-key-env .env.public_key

x-secret-key-env: &secret-key-env .env.secret_key

x-elastic-client-env: &elastic-client-env
  ELASTIC_HOST: ${ELASTIC_HOST:-elastic}
  ELASTIC_PORT: ${ELASTIC_PORT:-9200}
  ES_HOST: ${ELASTIC_HOST:-elastic}
  ES_PORT: ${ELASTIC_PORT:-9200}

x-etcd-client-env: &etcd-client-env
  ETCD_HOST: ${ETCD_HOST:-etcd}
  ETCD_PORT: ${ETCD_PORT:-2379}

x-influxdb-api-key: &influxdb-api-key .env.influxdb

x-influxdb-client-env: &influxdb-client-env
  INFLUX_BUCKET: ${INFLUX_BUCKET:-place}
  INFLUX_HOST: ${INFLUX_HOST:-http://influxdb:8086}
  INFLUX_ORG: ${INFLUX_ORG:-PlaceOS}

x-opentelemetry-env: &opentelemetry-env
  OTEL_EXPORTER_OTLP_HEADERS: ${OTEL_EXPORTER_OTLP_HEADERS:-}
  OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT:-}
  OTEL_EXPORTER_OTLP_API_KEY: ${OTEL_EXPORTER_OTLP_API_KEY:-}

x-place-loader-client-env: &place-loader-client-env
  PLACE_LOADER_URI: ${PLACE_LOADER_URI:-http://frontend-loader:3000}

x-redis-client-env: &redis-client-env
  REDIS_URL: ${REDIS_URL:-redis://redis:6379}

x-postgresdb-client-env: &postgresdb-client-env
  PG_HOST: ${PG_HOST:-postgres}
  PG_PORT: ${PG_PORT:-5432}
  PG_DB: ${PG_DB:-place_development}
  PG_USER: ${POSTGRES_USER:-postgres}
  PG_PASSWORD: ${POSTGRES_PASSWORD:-password}
  PG_DATABASE: ${PG_DB:-place_development}
  PG_DATABASE_URL: ${PG_DATABASE_URL:-postgresql://placeos:development@postgres:5432/place_development}


x-search-ingest-client-env: &search-ingest-client-env
  PLACE_SEARCH_INGEST_URI: ${PLACE_SEARCH_INGEST_URI:-http://search-ingest:3000}
  RUBBER_SOUL_URI: ${RUBBER_SOUL_URI:-http://rubber-soul:3000}

x-search-ingest-service: &search-ingest-service ${PLACE_SEARCH_INGEST_IMAGE:-search-ingest}

x-smtp-client-env: &smtp-client-env
  SMTP_SERVER: ${SMTP_SERVER:-}
  SMTP_PORT: ${SMTP_PORT:-587}
  SMTP_USER: ${SMTP_USER:-} # username if required, will not authenticate if blank
  SMTP_PASS: ${SMTP_PASS:-} # password if required
  SMTP_SECURE: ${SMTP_SECURE:-} # blank for unsecure, `SMTPS` for TLS, `STARTTLS` for negotiating TLS on unsecure connection

x-network: &std-network
  networks:
    placeos:

x-logging: &std-logging
  logging:
    driver: json-file
    options:
      max-size: 99m

services:
  # Services

  rest-api: # Rest API
    image: ${PLACE_REST_API_REGISTRY:-docker.io}/placeos/rest-api:${PLACE_REST_API_TAG:-latest}
    restart: always
    container_name: rest-api
    hostname: api # Retained as nginx upstream is `api`
    <<: *std-network
    <<: *std-logging
    depends_on:
      - auth
      - elastic
      - etcd
      - redis
      - postgres
      - search-ingest
    <<: *cap-drop
    env_file:
      - *jwt-public-key-env
      - *secret-key-env
    environment:
      <<: *deployment-env
      <<: *opentelemetry-env
      # Service Hosts
      <<: *elastic-client-env
      <<: *etcd-client-env
      <<: *place-loader-client-env
      <<: *redis-client-env
      <<: *postgresdb-client-env
      <<: *search-ingest-client-env
    deploy:
      resources:
        limits:
          memory: "2g"

  auth: # Authentication Service
    image: ${PLACE_AUTH_REGISTRY:-docker.io}/placeos/auth:${PLACE_AUTH_TAG:-latest}
    restart: always
    container_name: auth
    hostname: auth
    <<: *std-network
    <<: *std-logging
    depends_on:
      - redis
      - postgres
    <<: *cap-drop
    env_file:
      - *secret-key-env
    environment:
      <<: *opentelemetry-env
      <<: *postgresdb-client-env
      <<: *redis-client-env
      COAUTH_NO_SSL: "true"
      TZ: $TZ
      PLACE_URI: https://${PLACE_DOMAIN:-localhost:8443}
    deploy:
      resources:
        limits:
          memory: "500m"

  core: # Module coordinator
    image: ${PLACE_CORE_REGISTRY:-docker.io}/placeos/core:${PLACE_CORE_TAG:-latest}
    init: true # An init process prevents leaking of PlaceOS Driver processes
    restart: always
    container_name: core
    hostname: core
    <<: *std-network
    <<: *std-logging
    depends_on:
      - etcd
      - redis
      - postgres
    volumes:
      - type: volume
        source: core-repositories
        target: /app/repositories/
      - type: volume
        source: core-drivers
        target: /app/bin/drivers/
    <<: *cap-drop
    env_file:
      - *secret-key-env
    ulimits:
      nofile: 40000
      core:
        soft: 0
        hard: 0
    environment:
      <<: *deployment-env
      <<: *opentelemetry-env
      # Service Hosts
      <<: *etcd-client-env
      <<: *redis-client-env
      <<: *postgresdb-client-env
    deploy:
      resources:
        limits:
          memory: "4g"

  edge: # Module coordinator
    image: ${PLACE_EDGE_REGISTRY:-docker.io}/placeos/edge:${PLACE_EDGE_TAG:-latest}
    restart: "no"
    container_name: edge
    hostname: edge
    <<: *std-network
    <<: *std-logging
    depends_on:
      - core
    <<: *cap-drop
    environment:
      <<: *deployment-env
      <<: *edge-env
    deploy:
      resources:
        limits:
          memory: "2g"

  frontend-loader: # Frontend deployment service
    image: ${PLACE_FRONTEND_LOADER_REGISTRY:-docker.io}/placeos/${PLACE_FRONTEND_LOADER_IMAGE:-frontend-loader}:${PLACE_FRONTEND_LOADER_TAG:-latest}
    restart: always
    container_name: frontend-loader
    hostname: frontend-loader
    <<: *std-network
    <<: *std-logging
    volumes:
      - type: volume
        source: www
        target: /app/www
    depends_on:
      - postgres
    <<: *cap-drop
    env_file:
      - *secret-key-env
    environment:
      <<: *deployment-env
      <<: *opentelemetry-env
      # Service Hosts
      <<: *postgresdb-client-env
      PLACE_LOADER_WWW: www
    deploy:
      resources:
        limits:
          memory: "500m"

  source:
    image: ${PLACE_SOURCE_REGISTRY:-docker.io}/placeos/source:${PLACE_SOURCE_TAG:-latest}
    profiles:
      - analytics
    restart: always
    container_name: source
    hostname: source
    <<: *std-network
    <<: *std-logging
    depends_on:
      - influxdb
      - redis
      - postgres
    <<: *cap-drop
    env_file:
      - *influxdb-api-key
      - *secret-key-env
    environment:
      <<: *deployment-env
      <<: *opentelemetry-env
      # Service Hosts
      <<: *influxdb-client-env
      <<: *redis-client-env
      <<: *postgresdb-client-env
    deploy:
      resources:
        limits:
          memory: "500m"

  triggers: # Conditional execution service
    image: ${PLACE_TRIGGERS_REGISTRY:-docker.io}/placeos/triggers:${PLACE_TRIGGERS_TAG:-latest}
    restart: always
    container_name: triggers
    hostname: triggers
    <<: *std-network
    <<: *std-logging
    depends_on:
      - core
      - etcd
      - redis
      - postgres
    <<: *cap-drop
    env_file:
      - *secret-key-env
    environment:
      <<: *deployment-env
      <<: *opentelemetry-env
      # Service Hosts
      <<: *etcd-client-env
      <<: *redis-client-env
      <<: *postgresdb-client-env
      <<: *smtp-client-env
    deploy:
      resources:
        limits:
          memory: "250m"

  staff-api: # Staff API
    image: ${PLACE_STAFF_API_REGISTRY:-docker.io}/placeos/staff-api:${PLACE_STAFF_API_TAG:-latest}
    container_name: staff-api
    hostname: staff # Retained as nginx upstream is `staff`
    restart: unless-stopped
    <<: *std-network
    <<: *std-logging
    depends_on:
      - rest-api
      - postgres
    <<: *cap-drop
    env_file:
      - *jwt-public-key-env
      - *secret-key-env
    environment:
      <<: *opentelemetry-env
      <<: *redis-client-env
      <<: *postgresdb-client-env
      SG_ENV: production
      STAFF_TIME_ZONE: $TZ
      PLACE_URI: "https://nginx"
      SSL_VERIFY_NONE: "true"
    deploy:
      resources:
        limits:
          memory: "2g"

  # Support

  init:
    image: ${PLACE_INIT_REGISTRY:-docker.io}/placeos/init:${PLACE_INIT_TAG:-latest}
    container_name: init
    restart: on-failure
    <<: *std-network
    <<: *std-logging
    depends_on:
      postgres:
        condition: service_healthy
      #- search-ingest
    <<: *cap-drop
    env_file:
      - *secret-key-env
    environment:
      <<: *deployment-env
      <<: *opentelemetry-env
      # User/Application Configuration
      PLACE_DOMAIN: ${PLACE_DOMAIN:-localhost:8443}
      PLACE_APPLICATION: ${PLACE_APPLICATION:-backoffice}
      PLACE_AUTH_HOST: ${PLACE_AUTH_HOST:-auth:8080}
      PLACE_USERNAME: ${PLACE_USERNAME:-Place Support (localhost:8443)}
      PLACE_EMAIL: ${PLACE_EMAIL:-support@place.tech}
      PLACE_PASSWORD: ${PLACE_PASSWORD:-development}
      PLACE_TLS: "${PLACE_TLS:-true}"
      # Service Hosts
      <<: *postgresdb-client-env
      <<: *elastic-client-env

  search-ingest: # PostgreSQL to Elasticsearch Service
    image: ${PLACE_SEARCH_INGEST_REGISTRY:-docker.io}/placeos/${PLACE_SEARCH_INGEST_IMAGE:-search-ingest}:${PLACE_SEARCH_INGEST_TAG:-latest}
    restart: always
    container_name: *search-ingest-service
    hostname: *search-ingest-service
    <<: *std-network
    <<: *std-logging
    depends_on:
      elastic:
        condition: service_healthy
      postgres:
        condition: service_healthy
    <<: *cap-drop
    environment:
      <<: *deployment-env
      <<: *opentelemetry-env
     # Service Hosts
      <<: *postgresdb-client-env
      <<: *elastic-client-env
    deploy:
      resources:
        limits:
          memory: "250m"

  dispatch: # Engine driver server registration and data routing
    image: ${PLACE_DISPATCH_REGISTRY:-docker.io}/placeos/dispatch:${PLACE_DISPATCH_TAG:-latest}
    restart: always
    container_name: dispatch
    hostname: dispatch
    depends_on:
      - nginx
    <<: *cap-drop
    env_file:
      - *secret-key-env
    <<: *std-network
    <<: *std-logging
    deploy:
      resources:
        limits:
          memory: "250m"

  # Resources

  postgres: # Database used by PgORM Models
    image: postgres:${POSTGRES_VERSION:-13-alpine}
    container_name: postgres
    hostname: postgres
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $POSTGRES_USER -d $PG_DB"]
      interval: 10s
      timeout: 5s
      retries: 5
    <<: *std-network
    <<: *std-logging
    volumes:
      - postgres-data:/var/lib/postgresql/data
    environment:
      <<: *postgresdb-client-env
      TZ: $TZ
      POSTGRES_USER: $POSTGRES_USER
      POSTGRES_PASSWORD: $POSTGRES_PASSWORD
      POSTGRES_DB: $PG_DB
    deploy:
      resources:
        limits:
          memory: "4g"

  elastic:
    image: elasticsearch:${ELASTIC_VERSION:-7.17.6}
    restart: always
    container_name: elastic
    hostname: elastic
    healthcheck:
      test: curl --silent --fail localhost:9200/_cat/health
      start_period: 1m
    <<: *std-network
    <<: *std-logging
    volumes:
      - type: volume
        source: elastic-data
        target: /usr/share/elasticsearch/data
    environment:
      bootstrap.memory_lock: "true"
      cluster.routing.allocation.disk.threshold_enabled: "false"
      discovery.type: single-node
      TZ: $TZ
    deploy:
      resources:
        limits:
          memory: "2g"

  etcd:
    image: quay.io/coreos/etcd:${ETCD_VERSION:-v3.5.4}
    restart: always
    container_name: etcd
    hostname: etcd
    healthcheck:
      test: etcdctl endpoint health
    <<: *std-network
    <<: *std-logging
    environment:
      ALLOW_NONE_AUTHENTICATION: "yes"
      ETCD_NAME: "etcd"
      ETCD_INITIAL_ADVERTISE_PEER_URLS: "http://etcd:2380"
      ETCD_LISTEN_PEER_URLS: "http://0.0.0.0:2380"
      ETCD_LISTEN_CLIENT_URLS: "http://0.0.0.0:2379"
      ETCD_ADVERTISE_CLIENT_URLS: "http://etcd:2379"
      ETCD_INITIAL_CLUSTER_TOKEN: "etcd-cluster"
      ETCD_INITIAL_CLUSTER=etcd: "http://etcd:2380"
      ETCD_INITIAL_CLUSTER_STATE: "new"
      TZ: $TZ
    deploy:
      resources:
        limits:
          memory: "1g"

  influxdb:
    image: influxdb:${INFLUXDB_IMAGE_TAG:-2.0.8-alpine}
    profiles:
      - analytics
    restart: always
    container_name: influx
    hostname: influx
    <<: *std-network
    <<: *std-logging
    healthcheck:
      test: influx bucket list
    volumes:
      - type: volume
        source: influx-data
        target: /root/.influxdbv2
    command: "--reporting-disabled"
    deploy:
      resources:
        limits:
          memory: "1g"

  chronograf:
    image: chronograf:${CHRONOGRAF_IMAGE_TAG:-1.9}
    profiles:
      - analytics
    restart: always
    container_name: chronograf
    hostname: chronograf
    <<: *std-network
    <<: *std-logging
    depends_on:
      - influxdb
    env_file:
      - *influxdb-api-key
      - .env.chronograf # Contains TOKEN_SECRET for OAuth integration
    environment:
      BASE_PATH: /analytics
      INFLUXDB_URL: $INFLUX_HOST
      INFLUXDB_ORG: $INFLUX_ORG
      GENERIC_CLIENT_ID: "COPY_ANALYTICS_CLIENT_ID_FROM_BACKOFFICE"
      GENERIC_CLIENT_SECRET: "COPY_ANALYTICS_CLIENT_SECRET_FROM_BACKOFFICE"
      GENERIC_AUTH_URL: "https://${PLACE_DOMAIN}/auth/oauth/authorize"
      GENERIC_TOKEN_URL: "https://${PLACE_DOMAIN}/auth/oauth/token"
      GENERIC_API_URL: "https://${PLACE_DOMAIN}/api/engine/v2/users/current"
      GENERIC_API_KEY: "email"
      GENERIC_SCOPES: "public"
      GENERIC_NAME: "PlaceOS"
      PUBLIC_URL: "https://${PLACE_DOMAIN}"
    deploy:
      resources:
        limits:
          memory: "250m"

  mosquitto:
    image: iegomez/mosquitto-go-auth:${MOSQUITTO_IMAGE_TAG:-latest}
    profiles:
      - analytics
    restart: always
    container_name: mosquitto
    hostname: mosquitto
    <<: *std-network
    <<: *std-logging
    depends_on:
      - nginx
    volumes:
      - ${PWD}/config/mosquitto/mosquitto.conf:/etc/mosquitto/mosquitto.conf
    environment:
      TZ: $TZ
    deploy:
      resources:
        limits:
          memory: "250m"

  nginx:
    image: ${PLACE_NGINX_REGISTRY:-docker.io}/placeos/nginx:${PLACE_NGINX_TAG:-latest}
    restart: always
    container_name: nginx
    hostname: nginx
    ports:
      - 8080:80
      - 8443:443
    healthcheck:
      # Check if the http port is open
      test: "bash -c ': &>/dev/null </dev/tcp/127.0.0.1/80'"
    <<: *std-network
    <<: *std-logging
    depends_on:
      - rest-api
      - auth
      - staff-api
    volumes:
      - type: volume
        source: www
        target: /etc/nginx/html/
        read_only: true
      - type: volume
        source: nginx-data
        target: /etc/nginx/ssl
      - ${PWD}/.htpasswd-kibana:/etc/nginx/.htpasswd-kibana
    env_file:
      - *jwt-public-key-env
      - *influxdb-api-key
    environment:
      TZ: $TZ
      PLACE_DOMAIN: ${PLACE_DOMAIN:-localhost:8443}
    deploy:
      resources:
        limits:
          memory: "250m"

  redis:
    image: eqalpha/keydb
    restart: always
    container_name: redis
    hostname: redis
    healthcheck:
      test: keydb-cli ping
    <<: *std-network
    <<: *std-logging
    volumes:
      - type: volume
        source: redis-data
        target: /data
    environment:
      TZ: $TZ
    deploy:
      resources:
        limits:
          memory: "250m"

  # Aggregates logs and forwards them to Elasticsearch.
  logstash:
    image: blacktop/logstash:${ELASTIC_VERSION:-7.10.2}
    profiles:
      - kibana
    restart: always
    container_name: logstash
    hostname: logstash
    <<: *std-network
    <<: *std-logging
    depends_on:
      - elastic
      - validate-logstash-config
    volumes:
      - ${PWD}/config/logstash/config:/config
      - ${PWD}/config/logstash/patterns:/opt/logstash/extra_patterns
    command: logstash -f /config

  # Run 'docker-compose run --rm validate-logstash-config' to quickly check the logstash config.
  validate-logstash-config:
    image: blacktop/logstash:${ELASTIC_VERSION:-7.10.2}
    profiles:
      - kibana
    restart: "no"
    container_name: validate-logstash
    <<: *std-network
    <<: *std-logging
    volumes:
      - ${PWD}/config/logstash/config:/config
    command: logstash -t -f /config

  # Sends all container json-file logs to logstash
  logspout:
    image: vincit/logspout-gelf:3.2.6-alpine
    profiles:
      - kibana
    restart: unless-stopped
    container_name: logspout
    hostname: logspout
    <<: *std-network
    <<: *std-logging
    depends_on:
      - logstash
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command: gelf://${LOGSTASH_HOST}:${LOGSTASH_PORT}

  kibana:
    image: blacktop/kibana:${ELASTIC_VERSION:-7.10.2}
    profiles:
      - kibana
    restart: always
    container_name: kibana
    hostname: kibana
    <<: *std-network
    <<: *std-logging
    depends_on:
      - elastic
    environment:
      <<: *elastic-client-env
      NODE_OPTIONS: "--max-old-space-size=200" # fixes memory leak (https://github.com/elastic/kibana/issues/5170)
      HTTPS_METHOD: "nohttp"
      ELASTICSEARCH_HOSTS: "http://${ELASTIC_HOST}:${ELASTIC_PORT}"
      SERVER_BASEPATH: "/${PLACE_METRICS_ROUTE}"
      SERVER_REWRITEBASEPATH: "true"
      SERVER_PUBLICBASEURL: "https://${PLACE_DOMAIN}/${PLACE_METRICS_ROUTE}"

  # Takes care of piling up Elasticsearch indices/logs. Can do many other things as well.
  # Set up a cron job that runs "docker-compose run --rm curator --config /config.yml /action-file.yml" every once in a while.
  curator:
    image: bobrik/curator:5.8.1
    profiles:
      - kibana
    container_name: curator
    hostname: curator
    <<: *std-network
    <<: *std-logging
    depends_on:
      - elastic
    volumes:
      - ${PWD}/config/curator/action-file.yml:/action-file.yml
      - ${PWD}/config/curator/config.yml:/config.yml

  # Gets metrics from host machine and send to elastic
  metricbeat:
    image: elastic/metricbeat:${ELASTIC_VERSION:-7.10.2}
    profiles:
      - metricbeat
    restart: unless-stopped
    container_name: metricbeat
    hostname: metricbeat
    user: root
    <<: *std-network
    <<: *std-logging
    depends_on:
      - elastic
    environment:
      <<: *elastic-client-env
    volumes:
      - /proc:/hostfs/proc:ro
      - /sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro
      - /:/hostfs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ${PWD}/config/metricbeat.yml:/usr/share/metricbeat/metricbeat.yml
    cap_add:
      - SYS_PTRACE
      - DAC_READ_SEARCH
    command: ["metricbeat", "-e", "--strict.perms=false", "-system.hostfs=/hostfs", "-E", "output.elasticsearch.hosts=[$ELASTIC_HOST:$ELASTIC_PORT]"]
